---
title: "Spotify ML Project, Part 2: Preprocessing"
date: 2021-02-28
tags: [Python, Statistics]
excerpt: "Preprocessing Part 1 output, including standardization and normalization"
mathjax: "true"

---
<img src="{{ site.url }}{{ site.baseurl }}/images/spotify.png" alt="Spotify logo">


# Overview
Tools utilized:
* Python


Python libraries used:
* [Pandas](https://pandas.pydata.org/docs/)
* [Sys](https://docs.python.org/3/library/sys.html)
* [Math](https://docs.python.org/3/library/math.html)
* [Matplotlib](https://matplotlib.org/)
* [Sklearn](https://scikit-learn.org/stable/)
* [Seaborn](https://seaborn.pydata.org/#:~:text=Seaborn%20is%20a%20Python%20data,attractive%20and%20informative%20statistical%20graphics.)
* [SciPy](https://www.scipy.org/)
<br />
<br />


# Introduction
As an output from part 1, we have a table that contains a set of songs from Spotify with features that contain various metrics and measures, including 'danceability', 'energy', and more.

<table class="table table-bordered table-hover table-condensed">
<thead><tr><th title="Field #1">artist</th>
<th title="Field #2">album</th>
<th title="Field #3">track_name</th>
<th title="Field #4">track_id</th>
<th title="Field #5">danceability</th>
<th title="Field #6">energy</th>
<th title="Field #7">key</th>
<th title="Field #8">loudness</th>
<th title="Field #9">mode</th>
<th title="Field #10">speechiness</th>
<th title="Field #11">instrumentalness</th>
<th title="Field #12">liveness</th>
<th title="Field #13">valence</th>
<th title="Field #14">tempo</th>
<th title="Field #15">duration_ms</th>
<th title="Field #16">time_signature</th>
<th title="Field #17">saved</th>
</tr></thead>
<tbody>
<tr>
<td>Dance Gavin Dance</td>
<td>Instant Gratification</td>
<td>Death Of A Strawberry</td>
<td>0ZcQUwVyXgpUepJsvgOYgk</td>
<td align="right">0.576</td>
<td align="right">0.955</td>
<td align="right">4</td>
<td align="right">-3.122</td>
<td>1</td>
<td align="right">0.0458</td>
<td align="right">0.0</td>
<td align="right">0.054</td>
<td align="right">0.752</td>
<td align="right">124.972</td>
<td align="right">250973</td>
<td align="right">4</td>
<td>1</td>
</tr>
<tr>
<td>Youth Fountain</td>
<td>Letters to Our Former Selves</td>
<td>Rose Coloured Glass</td>
<td>5W1hGiWjGuQHoqd30nUkZR</td>
<td align="right">0.389</td>
<td align="right">0.984</td>
<td align="right">0</td>
<td align="right">-3.159</td>
<td>0</td>
<td align="right">0.235</td>
<td align="right">0.000187</td>
<td align="right">0.34</td>
<td align="right">0.617</td>
<td align="right">190.055</td>
<td align="right">197250</td>
<td align="right">4</td>
<td>0</td>
</tr>
</tbody></table>

<br />

In order to prepare the data for machine learning, standardization and normalization are necessary parts of the preprocessing....well, process.

Although in Part 3, Orange Data Mining software will be used (And makes preprocessing data a breeze!), completing this process in Python is an easy task and allows for a bit more customization and automation with the right code.

## Methodology
### Standardization
The first step is to standardize the data. While both standardization and normalization need to happen, often times when normalization is completed first, your data's scale can be thrown off, defeating the point of the process.

Instead of creating my own function, I found a great function that was created by 'DataMadness' on GitHub, available [here](https://datamadness.github.io/Skewness_Auto_Transform).

This function takes a dataframe as an input, as well as a few other inputs that allow you to tune the behavior of the function. It allows you to set a threshold for skewness, gives you the option to include/exclude columns, as well as allows you to decide between log/exponential transformation, or Box-Cox transformation. This is such a great, automated way to transform data quickly and easily!

``` python
def skew_autotransform(DF, include = None, exclude = None, plot = False, threshold = 1, exp = False):
    trans = DF
    #Get list of column names that should be processed based on input parameters
    if include is None and exclude is None:
        colnames = trans.columns.values
    elif include is not None:
        colnames = include
    elif exclude is not None:
        colnames = [item for item in list(trans.columns.values) if item not in exclude]
    else:
        print('No columns to process!')
    #Helper function that checks if all values are positive
    def make_positive(series):
        minimum = np.amin(series)
        #If minimum is negative, offset all values by a constant to move all values to positive teritory
        if minimum <= 0:
            series = series + abs(minimum) + 0.01
        return series
    #Go through desired columns in DataFrame
    for col in colnames:
        #Get column skewness
        skew = trans[col].skew()
        transformed = True
        if plot:
            #Prep the plot of original data
            sns.set_style("darkgrid")
            sns.set_palette("Blues_r")
            fig, axes = plt.subplots(1, 2, figsize=(10, 5))
            ax1 = sns.distplot(trans[col], ax=axes[0])
            ax1.set(xlabel='Original ' + col)
        #If skewness is larger than threshold and positively skewed; If yes, apply appropriate transformation
        if abs(skew) > threshold and skew > 0:
            skewType = 'positive'
            #Make sure all values are positive
            trans[col] = make_positive(trans[col])
            if exp:
               #Apply log transformation
               trans[col] = trans[col].apply(math.log)
            else:
                #Apply boxcox transformation
                trans[col] = ss.boxcox(trans[col])[0]
            skew_new = trans[col].skew()
        elif abs(skew) > threshold and skew < 0:
            skewType = 'negative'
            #Make sure all values are positive
            trans[col] = make_positive(trans[col])
            if exp:
               #Apply exp transformation
               trans[col] = trans[col].pow(10)
            else:
                #Apply boxcox transformation
                trans[col] = ss.boxcox(trans[col])[0]
            skew_new = trans[col].skew()
        else:
            #Flag if no transformation was performed
            transformed = False
            skew_new = skew
        #Compare before and after if plot is True
        if plot:
            print('\n ------------------------------------------------------')     
            if transformed:
                print('\n %r had %r skewness of %2.2f' %(col, skewType, skew))
                print('\n Transformation yielded skewness of %2.2f' %(skew_new))
                sns.set_palette("Paired")
                ax2 = sns.distplot(trans[col], ax=axes[1], color = 'r')
                ax2.set(xlabel='Transformed ' + col)
                plt.savefig(WD + '/SpotTrans/' + col +'.png')
                plt.show()
            else:
                print('\n NO TRANSFORMATION APPLIED FOR %r . Skewness = %2.2f' %(col, skew))
                ax2 = sns.distplot(trans[col], ax=axes[1])
                ax2.set(xlabel='NO TRANSFORM ' + col)
                plt.savefig(WD + '/SpotTrans/' + col +'.png')
                plt.show()
    return trans
```
In order to use this function, our output from Step 1 is read into a Pandas data frame, then passed to the function. Note that the DF passed to the function is actually a deep copy of the original DF imported with Pandas. This is done to prevent overwriting of the original data.
```python
tracks = pd.read_csv('tracks.csv')
standtracks = skew_autotransform(tracks.copy(deep = True), include = numeric_cols, plot = True, threshold = 1)
```
Since the threshold is set at 1, the function automatically applied Box Cox transformations to any feature that had a skewness above 1 or below -1. Take a look at the distributions for each feature; On the left is the original distribution, and on the right is the final distribution after standardization.
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/danceability.png" alt="danceability before and after">
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/duration_ms.png" alt="duration before and after">
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/energy.png" alt="energy before and after">
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/instrumentalness.png" alt="instrumentalness before and after">
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/key.png" alt="key before and after">
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/liveness.png" alt="liveness before and after">
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/loudness.png" alt="loudness before and after">
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/mode.png" alt="mode before and after">
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/speechiness.png" alt="speechiness before and after">
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/tempo.png" alt="tempo before and after">
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/valence.png" alt="valence before and after">


### Normalization
Now that all of our data is standardized, normalizing the data will help us get more consistent results in our Machine Learning models.
Let's take a look at some box and whisker plots of our numeric features to see how they all compare to one another.
Using Seaborn and Matplotlib, the following code outputs a combined box and whisker plot of the standardized data.
```python
sns.set_palette('plasma')
sns.set_context('talk', font_scale = 0.75)
fig = standtracks[features_list].plot(kind = 'box', figsize = (20,10))
plt.savefig('Standardized.png')
```
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/Standardized.png" alt="box and whisker before">

Next, to normalize each column, I created a normalization function that utilizes a simple normalization equation:
\\[ (X-Xmin)/(Xmax-Xmin)\\]
In the below function, it applies the above equation to columns that are not strings or are not our target variable, 'saved'.
```python
def normalize(df):
    result = df.copy()
    for col in df:
        if type(df[col][1]) != str and col != 'saved':
            max_val = df[col].max()
            min_val = df[col].min()
            result[col] = (df[col] - min_val) / (max_val - min_val)
            print(col, ' -- ', '*applied*')
        else:
            print(col, ' -- ', 'not applied')
            pass
    return result
```
When the standardized dataframe is passed to the function, it prints the column values and lets us know which columns were normalized, and which were not!
```python
proctracks = normalize(standtracks)
```
> artist  --  not applied <br />
> album  --  not applied <br />
> track_name  --  not applied <br />
> track_id  --  not applied <br />
> danceability  --  *applied* <br />
> energy  --  *applied* <br />
> key  --  *applied* <br />
> loudness  --  *applied* <br />
> mode  --  *applied* <br />
> speechiness  --  *applied* <br />
> instrumentalness  --  *applied* <br />
> liveness  --  *applied* <br />
> valence  --  *applied* <br />
> tempo  --  *applied* <br />
> duration_ms  --  *applied* <br />
> time_signature  --  *applied* <br />
> saved  --  not applied <br />

Using the below code, we are able to take a look at the box and whisker plots for the normalized and standardized dataset. Much better!
```python
sns.set_palette('plasma')
sns.set_context('talk', font_scale = 0.75)
fig = proctracks[features_list].plot(kind = 'box', figsize = (20,10))
plt.savefig('processed.png')
```
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/processed.png" alt="box and whisker after">

# Output
It doesn't seem like we did a lot in this part, but the difference it will make in our machine learning models in Part 3 will be invaluable!
Our output from this section is a new data table, in the exact same format, with standardized and normalized values.

Take a look at my complete code [here](https://tyljporter.github.io/SpotifyP2/).
