---
title: "Web Scraping, Sentiment Classification Modeling, and Topic Modeling Project"
date: 2021-03-15
tags: [Python, Text Analysis]
excerpt: "Web scraping of news.bitcoin.com articles, extraction of sentences, classification modeling, topic modeling."
mathjax: "true"

---
<img src="{{ site.url }}{{ site.baseurl }}/images/bitcoin.png" alt="Bitcoin logo">


# Overview
Tools utilized:
* Python


Python libraries used:
* [Pandas](https://pandas.pydata.org/docs/)
* [Sys](https://docs.python.org/3/library/sys.html)
* [NLTK](https://www.nltk.org/)
* [Textblob](https://textblob.readthedocs.io/en/dev/)
* [Sklearn](https://scikit-learn.org/stable/)
* [BS4 Beautiful Soup](https://pypi.org/project/beautifulsoup4/)
* OS
<br />
<br />


# Introduction

As part of a course-end project in my Text Analytics class, my group and I (colleagues Troy Parker and Praveen Gadkari), had to come up with a project that encapsulated everything we had learned over the semester.

The topics to be covered were as follows: Web scraping, sentiment classification, and cluster & topic analysis.

In choosing a topic to cover, the requirements were simple; there needed to be over 5,000 articles on the website, and the topic had to be timely and relevant. In 2021, cryptocurrencies have been making big headlines. Troy, a part-time scuba diving instructor, had just gotten back from an out-of-country diving trip where a self-made crypto millionaire chartered him to instruct a private group of people.

Here is a recording of our class presentation, if you prefer to listen rather than read!
<iframe width="560" height="315" src="https://www.youtube.com/embed/vJUBGqf90s0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


# Methodology

### Site Scraping
With our site selected, https://news.bitcoin.com/, the first task was to scrape the site for articles. This was done with the following code:
```python
#Set url
url = "https://news.bitcoin.com/"
#Get target website
r = requests.get(url)
soup = BeautifulSoup(r.content, "html.parser")
#make file path names

#number of pages(500 x 8 articles = 4000 web pages)
numPages = 500

#page 1 of website
urls = ["https://news.bitcoin.com/"]

#make each additional website path
for i in range(2, numPages):     # The range(2, numPages) generates a list of integers from 2 to 9.
    url = "https://news.bitcoin.com/page/{}/".format(i)
    urls.append(url)

#Create directory for html files
path = "HTML_Files/Bitcoin5/"
#If path doesn't exist, create
if not os.path.isdir(path):
    os.mkdir(path)

#load web pages to directory

for url in urls:
    print(url)

    ####################################################
    # Get the content of a page
    ####################################################
    r = requests.get(url)
    soup = BeautifulSoup(r.content, "html.parser")

    ####################################################
    # Get the list of articles
    ####################################################
    h_list = soup.find_all(name="div", attrs={"class": "story story--medium"})

    for h in h_list:
        ####################################################
        # Find the anchor tag
        ####################################################
        a = h.find("a")
        b = h.find("h6")

        ####################################################
        # Extract the title & URL of an article
        ####################################################
        title = b.text
        article_url = a["href"]

        ####################################################
        # Fetch the content and save it as an HTML file
        ####################################################
        r2 = requests.get(article_url)

        file_name = article_url[len("https://news.bitcoin.com/"):-1] + ".html"
        with open(path + file_name, "w+b") as fw:
            fw.write(r2.content)

        print("- " + file_name + " saved.")

        ####################################################
        # Sleep for a second to not overload the web site
        ####################################################
        time.sleep(1)

```
After the above block of code is ran, we now have almost 5,000 html pages scraped and saved in one central location.

Now, in order to get all of the .html files into a .csv file for manual coding, the following code block was utilized:

```python
#create csv file from html files previously downloaded

with open("CSV_File/html_metadata.csv", "w", encoding="utf8") as fw:
    ####################################################
    # Column names on the first row
    ####################################################
    fw.write("file_name\tarticle_title\tarticle_date_time\tarticle_author\tarticle_text\n")   # A tab between columns and a new line between rows  

    for file_name in os.listdir(path):
        if not file_name.endswith(".html"):
            continue

        ####################################################
        # Column values starting from the second row
        ####################################################
        with open(path + file_name, "r+b") as fr:
            print(file_name)
            soup = BeautifulSoup(fr.read(), "html.parser")
            article_title = soup.find(name="article", attrs={"class": "article__body"}).find("h1").text.strip()

            ##modified by Troy -- this area contains the date of the article
            article_date_time = soup.find("main",attrs={"article full-grid"}).find("time").text.strip()

            article_text = soup.find(name="article", attrs={"class": "article__body"}).text.replace("\n", " ").replace("\t", " ").strip()

            ####################################################
            # No author exception handling
            ####################################################
            if soup.find(name="p", attrs={"class": "article__info__author"}) == None:
                article_author = ""
            else:
                article_author = soup.find(name="p", attrs={"class": "article__info__author"}).find("strong").text.strip()


            #####################################################################
            # Remove all possible tabs, as tab is being used as column delimiter
            #####################################################################
            article_title = article_title.replace("\t", "")
            article_date_time = article_date_time.replace("\t", "")

            #article_date_time = get_past_date(article_days_ago)

            article_author = article_author.replace("\t", "")
            article_text = article_text.replace("\t", "")

            fw.write("{}\t{}\t{}\t{}\t{}\n".format(file_name, article_title, article_date_time, article_author, article_text))
```

After the completion of the above code, we now have a .csv file with the name of the file, the article title, the date and time of posting, the author, and the article text. This data is ready for further processing into sentences for manual sentiment coding.

### Text Preparation and Output for Manual Coding

At this point in the process, we need to take the .csv file of article text and tokenize them into individual sentences. Made easy through the nltk package's 'sent_tokenize' function, the following line of code outputs a new panda's column in the dataframe that corresponds to a list of tokenized sentences extracted from the article_text column.
```python
df["sentences"] = df.article_text.apply(lambda x: nltk.sent_tokenize(x))
```

With a total of 161,198 sentences, a random sample needed to be selected. The following code was used to accomplish this:
```python
#Get 2000 random sentences
Random_Sentences = []
Unique_Sentences = []
pick = 2000
i = 0
while i < (pick):
    Random_Sentences.extend(random.sample(All_Sentences, 1))
    Unique_Sentences = set(Random_Sentences)
    i = len(Unique_Sentences)
Random_Sentences = list(Unique_Sentences)
```

The random sentences are then merged, duplicates removed, and output to a .csv file using Pandas.

```python
#merge list of 2000 sentences to data frame for export
dfSent = DataFrame(Random_Sentences,columns=['Sentence'])
#check for duplicates
dfSent[dfSent.Sentence.duplicated(keep="last")]
#Export to CSV to mannually classify
dfSent.to_csv("RandomSentences/Sentences2.csv",sep='\t')
```

In order to get the most accurate coding between Praveen, Troy, and I, we did some research and devised the following process for manual sentiment classification: If a sentence has more than one sentiment present, all positive, negative, and neutral adjectives and adverbs were added together and the corresponding category with the highest net count "won" the overall categorization. Sarcasm was treated as positive sentiment, as in American English, sarcasm contains positive words about a negative situation. No clear sentiment was coded as neutral.

### Sentiment Classifier Prep

With a .csv file of manually coded sentences ready to be used as training data, the first step is importing the file with Pandas.

```python
#import manually classified sentences
dfSent = pd.read_csv("RandomSentences/SentencesTab.csv", sep='\t', encoding = 'unicode_escape')
```

To prepare our data to be used in ML models, the first step is to utilize a sklearn vectorizer. Our X dataset will be our sentences, and our Y dataset will be our manual classifications. For this project, we will use a standard set of english stopwords as well as l2 normalization with a test size of 20%.

```python
#import vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(use_idf=True, norm="l2", stop_words="english", max_df=0.7)
X = vectorizer.fit_transform(dfSent.Sentence)
y = dfSent.Class

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
```

In order to get a clear output of the train and test scores, a custom function was created that is passed the test and training data as well as a selected model from sklearn. This function prints a confusion matrix as well as the training and test scores for each model evaluation. We will use this as a visualization during iterative tuning of selected models!

```python
#function for classification report
from sklearn.metrics import classification_report, confusion_matrix

def train_test(X_train, X_test, y_train, y_test, classifier):
    classifier.fit(X_train, y_train)
    pred = classifier.predict(X_test)

    print("Train score: {:.2f}".format(classifier.score(X_train, y_train)))
    print("Test score: {:.2f}\n".format(classifier.score(X_test, y_test)))
    print("Classification report:\n{}".format(classification_report(y_test, pred, zero_division=0)))
    print(confusion_matrix(y_test,pred))

    return classifier
```

In order to "test" the models, a sample of sentences with positive, negative, and neutral sentiments were selected from various financial news sources.

```python
# Prediction cases
#neutral
text1 = 'Rothbard writes: “Suppose the economy has a supply of 10,000 gold ounces, and counterfeiters [the state]…pump in 2000 ‘ounces’ more.'
text2 = 'The group revealed: Nearly 100 countries carried out automatic exchange of information in 2019, enabling their tax authorities to obtain data on 84 million financial accounts held offshore by their residents, covering total assets of EUR 10 trillion.'
text3 = 'The problem arose while he was in Iran as he says he simply looked at his account but performed no transactions or made no transfers during the checkup.'
#positive
text4 = '“Currently Bitcoin appears to be struggling to stay above its support, however, it is showing some bullish divergences so I’d expect a rally in the short term,” Pelecanos noted.'
text5 = 'Data collected by blockchain forensics company Chainalysis shows a significant increase of volume in the first half of the year.'
text6 = 'Already, Mastercard has announced it will be supporting cryptocurrencies in its payments network.'
#negative
text7 = 'Looming US Real Estate Crisis - Freddie Mac Warns of Housing Market Uncertainty, Homebuilder sentiment Drops 58%                        U.S. real estate agents and lenders are bracing for the biggest housing crash in over a decade.'
text8 = 'After a rogue employee deleted President Donald Trump’s account two years ago, the company limited access to national leaders’ accounts to a much smaller number of people.'
text9 = 'A small number of people are starting to focus on the economic situation and the possible destructive aftermath the finance world will see.'
text10 = 'If they eventually dump on you, there is no way you will recover.'
#vectorize prediction texts
new_texts = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10]
X_new = vectorizer.transform(new_texts)
```

### Sentiment Classifier Model Selection
Now that we have our data set up and a function defined to help with training and testing, we will evaluate a multitude of different models for accuracy:
1. K-Nearest Neighbors
2. Logistic Regression
3. Multnomial Naive Bayes
4. Linear Support Vector Machines
5. Kernelized Support Vector Machines
6. Neural Networks
7. Random Forest

Without digging too deep into this section of code, the above models were ran with iteratively selected parameters, and the best score was selected and output to a summary dataframe, rounded to 3 decimal points.
Once the best paramaters are saved, and the score is output, the model is used to predict the sentiment classification for the above sentences. Here is a sample of code, from the KNN Model:

```python
#KNN model
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score


score_max = 0                      
for param in [1, 3, 5, 7, 9, 11]:
    model = KNeighborsClassifier(n_neighbors=param)
    scores = cross_val_score(model, X_train, y_train, cv=10)
    print("k = {}: {}\n{:.3f}, {:.3f}\n".format(param, scores, scores.mean(), scores.std()))

    if scores.mean() > score_max:
        score_max = scores.mean()
        param_best = param         

print("Highest score : {:.3f} when k = {}".format(score_max, param_best))

print("k = {}".format(param_best))
knn = KNeighborsClassifier(n_neighbors=param_best)
knn = train_test(X_train, X_test, y_train, y_test, knn)
summary["KNN"] = round(knn.score(X_test, y_test), 3)
knn_pred = knn.predict(X_new)
```

After the tuning and predictions are completed for every model, the results are shown below using the following code:

```python
pd.DataFrame.from_dict(summary, orient = 'index', columns = ['Score'])
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Logistic Regression</th>
      <td>0.525</td>
    </tr>
    <tr>
      <th>Multinomial Naive Bayes</th>
      <td>0.518</td>
    </tr>
    <tr>
      <th>Linear SVMs</th>
      <td>0.528</td>
    </tr>
    <tr>
      <th>Kernelized SVMs</th>
      <td>0.502</td>
    </tr>
    <tr>
      <th>Neural Networks</th>
      <td>0.502</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>0.502</td>
    </tr>
    <tr>
      <th>KNN</th>
      <td>0.420</td>
    </tr>
  </tbody>
</table>
</div>

Overall, the best model is the Linear SVM model with Logistic Regression coming in a close second.
The worst model, KNN, was only marginally worse with a 22.8% difference in score.

When comparing these models against random-chance accuracy, a random guess of sentiment would have a 33.33% change of being accurate; with this in mind, our worst model(KNN) is 8.6% more accurate than random guessing, and the best model(Linear SVM) is 19.2% more accurate than guessing.

```python
#Create dataframe of model predictions
models = [knn_pred, lr_pred, mnb_pred, lsvm_pred, ksvm_pred, nn_pred, rf_pred]
predictions = pd.DataFrame.from_records(models).transpose()
predictions.columns = ['KNN', 'LR', 'MNB', 'Lin. SVM', 'Kern. SVM', 'Neur. Net', 'Rand. For.']
#Add predicted sentences into dataframe
predictions['sentence'] = new_texts
predictions['sent'] = ['neutral', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative']
predictions
```
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>KNN</th>
      <th>LR</th>
      <th>MNB</th>
      <th>Lin. SVM</th>
      <th>Kern. SVM</th>
      <th>Neur. Net</th>
      <th>Rand. For.</th>
      <th>sentence</th>
      <th>sent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>neutral</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>Rothbard writes: “Suppose the economy has a supply of 10,000 gold ounces, and counterfeiters [the state]…pump in 2000 ‘ounces’ more.</td>
      <td>neutral</td>
    </tr>
    <tr>
      <th>1</th>
      <td>neutral</td>
      <td>neutral</td>
      <td>positive</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>The group revealed: Nearly 100 countries carried out automatic exchange of information in 2019, enabling their tax authorities to obtain data on 8...</td>
      <td>neutral</td>
    </tr>
    <tr>
      <th>2</th>
      <td>neutral</td>
      <td>negative</td>
      <td>positive</td>
      <td>negative</td>
      <td>negative</td>
      <td>positive</td>
      <td>neutral</td>
      <td>The problem arose while he was in Iran as he says he simply looked at his account but performed no transactions or made no transfers during the ch...</td>
      <td>nautral</td>
    </tr>
    <tr>
      <th>3</th>
      <td>neutral</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>“Currently Bitcoin appears to be struggling to stay above its support, however, it is showing some bullish divergences so I’d expect a rally in th...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>4</th>
      <td>neutral</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>Data collected by blockchain forensics company Chainalysis shows a significant increase of volume in the first half of the year.</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>5</th>
      <td>neutral</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>neutral</td>
      <td>Already, Mastercard has announced it will be supporting cryptocurrencies in its payments network.</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>6</th>
      <td>neutral</td>
      <td>negative</td>
      <td>negative</td>
      <td>negative</td>
      <td>negative</td>
      <td>negative</td>
      <td>positive</td>
      <td>Looming US Real Estate Crisis - Freddie Mac Warns of Housing Market Uncertainty, Homebuilder sentiment Drops 58%                        U.S. real ...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>7</th>
      <td>neutral</td>
      <td>negative</td>
      <td>positive</td>
      <td>negative</td>
      <td>negative</td>
      <td>negative</td>
      <td>negative</td>
      <td>After a rogue employee deleted President Donald Trump’s account two years ago, the company limited access to national leaders’ accounts to a much ...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>8</th>
      <td>neutral</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>positive</td>
      <td>A small number of people are starting to focus on the economic situation and the possible destructive aftermath the finance world will see.</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>9</th>
      <td>neutral</td>
      <td>neutral</td>
      <td>positive</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>If they eventually dump on you, there is no way you will recover.</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>


This view allows us to compare the predictions across the various models by each sentence.
As you can see, the models did well in predicting some sentences, such as index '0'.
Another example is index position '4'--KNN, our worst model, was the only one that predicted incorrectly.
In index position '9', it is interesting to note that all models predicted this incorrectly.

If given more time, in order to increase the accuracy of our models, a first step would be to re-visit our coding technique to create a process that would allow three coders to be in further agreement; if all parties rated the same sentences, then came together to dispute coding differences, we may have a much more consistent rating that is used to train the models.


### Cluster & Topic Analysis
Using the entire set of 4,498 articles, my team and I used K-Means clustering to group together articles with similar intents. We tried different values for K from 2 to 6. K=4 seemed to make the most sense because the others had lopsided number of records with the largest cluster containing most of the data (even more than when K=4). Besides that, the grouping did not make sense with a lot of jumbled up data in those clusters. We could draw almost no inference from them.
See our clustering code below:

```python
#Step 1: Choose the number of clusters
k = 4
#Step 2: Initialize the model object for k-means clustering
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=k, random_state=0)
kmeans
#Step 3: Fit the model using the input data
%time kmeans.fit(X)
#Step 4: Examine the clustering outcome
df["cluster"] = kmeans.labels_
df[["article_title", "cluster"]]
```

A custom counter function was defined to collect and count common words within clusters. the second cluster is printed as an example!

```python
#checking top words by cluster
import nltk
df["words"] = df.article_title.apply(lambda x: nltk.word_tokenize(x))
df["tagged_words"] = df.words.apply(lambda x: nltk.pos_tag(x))

from collections import Counter

def get_counter(dataframe, stopwords=[]):
    counter = Counter()

    for l in dataframe.tagged_words:
        word_set = set()

        for t in l:
            word = t[0].lower()
            tag = t[1]

            if word not in stopwords:
                word_set.add(word)

        counter.update(word_set)

    return counter

#Second  cluster - Theme around market performance/pricing
counter0 = get_counter(df[df.cluster == 0], global_stopwords+local_stopwords)
counter0.most_common(30)
```

    [('mining', 144),
     ('price', 106),
     ('government', 93),
     ('markets', 79),
     ('gold', 78),
     ('cryptocurrencies', 78),
     ('indian', 75),
     ('miners', 73),
     ('exchanges', 72),
     ('worth', 68),
     ('network', 67),
     ('defi', 66),
     ('money', 66),
     ('report', 63),
     ('investors', 63),
     ('–', 63),
     ('ethereum', 62),
     ('year', 61),
     ('court', 61),
     ('buy', 59),
     ('satoshi', 59),
     ('china', 58),
     ('wallet', 57),
     ('could', 55),
     ('tax', 54),
     ('fund', 54),
     ('shows', 53),
     ('2020', 53),
     ('payments', 50),
     ('sec', 50)]

The next step is LDA Topic Modeling using sklearn. Our group chose 4 topics for readability. After the model was initialized, a custom function was created to print the topic score as well as the feature name for each feature per topic.

```python
#step 1: set the number of topics
num_topics = 4
#step 2: Initialize a model for LDA topic modeling
from sklearn.decomposition import LatentDirichletAllocation as LDA
lda = LDA(n_components=num_topics, random_state=0)
#step 3: fit the model
%time lda.fit(X)
#step 4: Examine the output topic

def show_topics(model, feature_names, num_top_words):
    for topic_idx, topic_scores in enumerate(model.components_):
        print("***Topic {}:".format(topic_idx))
        print(" + ".join(["{:.2f} * {}".format(topic_scores[i], feature_names[i]) for i in topic_scores.argsort()[::-1][:num_top_words]]))
        print()

show_topics(lda, vectorizer.get_feature_names(), 10)
```

***Topic 0:
13.01 * mining + 11.60 * 000 + 9.98 * token + 9.86 * market + 9.76 * scam + 8.21 * exchange + 7.93 * prices + 7.32 * markets + 7.22 * video + 6.99 * miners

***Topic 1:
12.97 * market + 12.51 * price + 12.12 * digital + 11.21 * bank + 10.77 * fees + 10.48 * satoshi + 9.53 * buy + 9.34 * fund + 9.12 * government + 8.92 * markets

***Topic 2:
21.29 * exchange + 16.65 * gold + 16.30 * trading + 15.43 * futures + 14.68 * bank + 14.66 * court + 13.87 * indian + 13.59 * market + 13.24 * platform + 11.90 * launches

***Topic 3:
15.24 * banks + 15.00 * bank + 14.92 * exchanges + 11.77 * exchange + 11.63 * tax + 9.97 * services + 9.50 * rates + 8.87 * interest + 8.56 * year + 8.53 * launches

The final step is to create a co-occurance graph of our most common words. The below code utilizes a custom counter, similar to what was shown above, to graph a network or word co-occurances.

```python
from collections import Counter
import networkx as nx
from matplotlib import pyplot as plt
###################################################################################
# The 'counter' object will have all the word count information.
# The 'co_counter' object will have all the co-occurrence count information.
###################################################################################
counter = Counter()
co_counter = dict()

for l in df.words:
    word_set = set()

    for item in l:
        word = item.lower()

        if word not in (global_stopwords + local_stopwords):
            word_set.add(word)

    counter.update(word_set)

    ###################################################################################
    # Calculate co-occurrence count of two words and save it in 'co_counter'.
    # Co_counter is a dictionary of dictionaries.
    ###################################################################################
    words = list(word_set)
    for word1 in words:
        if word1 not in co_counter:
            co_counter[word1] = dict()

        for word2 in words:
            ######################################
            # Skip if the two words are the same.
            ######################################
            if word1 == word2:
                continue

            if word2 not in co_counter[word1]:
                co_counter[word1][word2] = 1
            else:
                co_counter[word1][word2] += 1


G = nx.Graph()
nodes = [item[0] for item in counter.most_common(20)]
node_weights = [item[1] * 15 for item in counter.most_common(20)]

for word in nodes:
    G.add_node(word, weight=counter.get(word))

for word1 in nodes:
    for word2 in nodes:
        if (word1 != word2) & (word2 in co_counter[word1]):
            G.add_edge(word1, word2, weight=co_counter[word1][word2])

edges = nx.get_edge_attributes(G, "weight").keys()
edge_weights = nx.get_edge_attributes(G, "weight").values()

edge_weights = [item / 5 for item in edge_weights]

plt.figure(figsize=(10, 10))
nx.draw_networkx(G, pos=nx.circular_layout(G),
                 nodelist=nodes, node_size=node_weights, edgelist=edges, width=edge_weights,
                 node_color="yellow", with_labels=True, font_size=10)
plt.draw()
```
<img src="{{ site.url }}{{ site.baseurl }}/images/graph.png" alt="word co-occurance graph">



Overall, the following summary applies:

1) There seems to be a resonable corelation between the clusters and the topic analysis when they both are set to 4.

2) The weighted words in each of the topics are aligned with what we observed through the cluster analysis.

3) Another interesting observation was the comparison between bitcoin and gold in cluster=2/topic=2. Random analysis of our sentences revealed that there was a discussion around the limited quantity of both bitcoin and gold. In both cases, their value is supported, in part, by scarcity. Gold is limited by physical supply and the difficulty of extraction, while bitcoin creation is capped at 21 million by its source code. These qualities, as well as the deep liquid markets, mean that both gold and bitcoin have the potential to retain value, and in fact appreciate, during difficult economic cycles.


Click [here](https://tyljporter.github.io/BitcoinPart1/) to see a complete view of my python code for Part 1, Scraping.
Click [here](https://tyljporter.github.io/BitcoinPart2/) to see a complete view of my python code for Part 2, Sentiment Classification.
Click [here](https://tyljporter.github.io/BitcoinPart3/) to see a complete view of my python code for Part 3, Cluster and Topic Analysis.
