---
title: "Web Scraping, Sentiment Classification Modeling, and Topic Modeling Project"
date: 2021-03-15
tags: [Python, Text Analysis]
excerpt: "Web scraping of news.bitcoin.com articles, extraction of sentences, classification modeling, topic modeling."
mathjax: "true"

---
<img src="{{ site.url }}{{ site.baseurl }}/images/bitcoin.png" alt="Bitcoin logo">


# Overview
Tools utilized:
* Python


Python libraries used:
* [Pandas](https://pandas.pydata.org/docs/)
* [Sys](https://docs.python.org/3/library/sys.html)
* [NLTK](https://www.nltk.org/)
* [Textblob](https://textblob.readthedocs.io/en/dev/)
* [Sklearn](https://scikit-learn.org/stable/)
* [BS4 Beautiful Soup](https://pypi.org/project/beautifulsoup4/)
* OS
<br />
<br />


# Introduction
As part of a course-end project in my Text Analytics class, my group and I (colleagues Troy Parker and Praveen Gadkari), had to come up with a project that encapsulated everything we had learned over the semester.

The topics to be covered were as follows: Web scraping, sentiment classification, and cluster & topic analysis.

In choosing a topic to cover, the requirements were simple; there needed to be over 5,000 articles on the website, and the topic had to be timely and relevant. In 2021, cryptocurrencies have been making big headlines. Troy, a part-time scuba diving instructor, had just gotten back from an out-of-country diving trip where a self-made crypto millionaire chartered him to instruct a private group of people.

## Methodology
### Site scraping
With our site selected, https://news.bitcoin.com/, the first task was to scrape the site for articles. This was done with the following code:
```Python
#Set url
url = "https://news.bitcoin.com/"
#Get target website
r = requests.get(url)
soup = BeautifulSoup(r.content, "html.parser")
#make file path names

#number of pages(500 x 8 articles = 4000 web pages)
numPages = 500

#page 1 of website
urls = ["https://news.bitcoin.com/"]

#make each additional website path
for i in range(2, numPages):     # The range(2, numPages) generates a list of integers from 2 to 9.
    url = "https://news.bitcoin.com/page/{}/".format(i)
    urls.append(url)

#Create directory for html files
path = "HTML_Files/Bitcoin5/"
#If path doesn't exist, create
if not os.path.isdir(path):
    os.mkdir(path)

#load web pages to directory

for url in urls:
    print(url)

    ####################################################
    # Get the content of a page
    ####################################################
    r = requests.get(url)
    soup = BeautifulSoup(r.content, "html.parser")

    ####################################################
    # Get the list of articles
    ####################################################
    h_list = soup.find_all(name="div", attrs={"class": "story story--medium"})

    for h in h_list:
        ####################################################
        # Find the anchor tag
        ####################################################
        a = h.find("a")
        b = h.find("h6")

        ####################################################
        # Extract the title & URL of an article
        ####################################################
        title = b.text
        article_url = a["href"]

        ####################################################
        # Fetch the content and save it as an HTML file
        ####################################################
        r2 = requests.get(article_url)

        file_name = article_url[len("https://news.bitcoin.com/"):-1] + ".html"
        with open(path + file_name, "w+b") as fw:
            fw.write(r2.content)

        print("- " + file_name + " saved.")

        ####################################################
        # Sleep for a second to not overload the web site
        ####################################################
        time.sleep(1)

```
Now, in order to get the .html into a csv file for manual coding, the following code block was utilized:

```Python
#create csv file from html files previously downloaded

with open("CSV_File/html_metadata.csv", "w", encoding="utf8") as fw:
    ####################################################
    # Column names on the first row
    ####################################################
    fw.write("file_name\tarticle_title\tarticle_date_time\tarticle_author\tarticle_text\n")   # A tab between columns and a new line between rows  

    for file_name in os.listdir(path):
        if not file_name.endswith(".html"):
            continue

        ####################################################
        # Column values starting from the second row
        ####################################################
        with open(path + file_name, "r+b") as fr:
            print(file_name)
            soup = BeautifulSoup(fr.read(), "html.parser")
            article_title = soup.find(name="article", attrs={"class": "article__body"}).find("h1").text.strip()

            ##modified by Troy -- this area contains the date of the article
            article_date_time = soup.find("main",attrs={"article full-grid"}).find("time").text.strip()

            article_text = soup.find(name="article", attrs={"class": "article__body"}).text.replace("\n", " ").replace("\t", " ").strip()

            ####################################################
            # No author exception handling
            ####################################################
            if soup.find(name="p", attrs={"class": "article__info__author"}) == None:
                article_author = ""
            else:
                article_author = soup.find(name="p", attrs={"class": "article__info__author"}).find("strong").text.strip()


            #####################################################################
            # Remove all possible tabs, as tab is being used as column delimiter
            #####################################################################
            article_title = article_title.replace("\t", "")
            article_date_time = article_date_time.replace("\t", "")

            #article_date_time = get_past_date(article_days_ago)

            article_author = article_author.replace("\t", "")
            article_text = article_text.replace("\t", "")

            fw.write("{}\t{}\t{}\t{}\t{}\n".format(file_name, article_title, article_date_time, article_author, article_text))
```

After the completion of the above code, we now have a .csv file with the name of the file, the article title, the date and time of posting, the author, and the article text. This data is ready for further processing!



### Sentiment Classifier
Now that all of our data is standardized, normalizing the data will help us get more consistent results in our Machine Learning models.
Let's take a look at some box and whisker plots of our numeric features to see how they all compare to one another.
Using Seaborn and Matplotlib, the following code outputs a combined box and whisker plot of the standardized data.
```python
sns.set_palette('plasma')
sns.set_context('talk', font_scale = 0.75)
fig = standtracks[features_list].plot(kind = 'box', figsize = (20,10))
plt.savefig('Standardized.png')
```
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/Standardized.png" alt="box and whisker before">

Next, to normalize each column, I created a normalization function that utilizes a simple normalization equation:
\\[ (X-Xmin)/(Xmax-Xmin)\\]
In the below function, it applies the above equation to columns that are not strings or are not our target variable, 'saved'.
```python
def normalize(df):
    result = df.copy()
    for col in df:
        if type(df[col][1]) != str and col != 'saved':
            max_val = df[col].max()
            min_val = df[col].min()
            result[col] = (df[col] - min_val) / (max_val - min_val)
            print(col, ' -- ', '*applied*')
        else:
            print(col, ' -- ', 'not applied')
            pass
    return result
```
When the standardized dataframe is passed to the function, it prints the column values and lets us know which columns were normalized, and which were not!
```python
proctracks = normalize(standtracks)
```
> artist  --  not applied <br />
> album  --  not applied <br />
> track_name  --  not applied <br />
> track_id  --  not applied <br />
> danceability  --  *applied* <br />
> energy  --  *applied* <br />
> key  --  *applied* <br />
> loudness  --  *applied* <br />
> mode  --  *applied* <br />
> speechiness  --  *applied* <br />
> instrumentalness  --  *applied* <br />
> liveness  --  *applied* <br />
> valence  --  *applied* <br />
> tempo  --  *applied* <br />
> duration_ms  --  *applied* <br />
> time_signature  --  *applied* <br />
> saved  --  not applied <br />

Using the below code, we are able to take a look at the box and whisker plots for the normalized and standardized dataset. Much better!
```python
sns.set_palette('plasma')
sns.set_context('talk', font_scale = 0.75)
fig = proctracks[features_list].plot(kind = 'box', figsize = (20,10))
plt.savefig('processed.png')
```
<img src="{{ site.url }}{{ site.baseurl }}/images/SpotTrans/processed.png" alt="box and whisker after">

# Output
It doesn't seem like we did a lot in this part, but the difference it will make in our machine learning models in Part 3 will be invaluable!
Our output from this section is a new data table, in the exact same format, with standardized and normalized values.

Take a look at my complete code [here](https://tyljporter.github.io/SpotifyP2/).
