---
title: "Web Scraping, Sentiment Classification Modeling, and Topic Modeling Project"
date: 2021-03-15
tags: [Python, Text Analysis]
excerpt: "Web scraping of news.bitcoin.com articles, extraction of sentences, classification modeling, topic modeling."
mathjax: "true"

---
<img src="{{ site.url }}{{ site.baseurl }}/images/bitcoin.png" alt="Bitcoin logo">


# Overview
Tools utilized:
* Python


Python libraries used:
* [Pandas](https://pandas.pydata.org/docs/)
* [Sys](https://docs.python.org/3/library/sys.html)
* [NLTK](https://www.nltk.org/)
* [Textblob](https://textblob.readthedocs.io/en/dev/)
* [Sklearn](https://scikit-learn.org/stable/)
* [BS4 Beautiful Soup](https://pypi.org/project/beautifulsoup4/)
* OS
<br />
<br />


# Introduction
As part of a course-end project in my Text Analytics class, my group and I (colleagues Troy Parker and Praveen Gadkari), had to come up with a project that encapsulated everything we had learned over the semester.

The topics to be covered were as follows: Web scraping, sentiment classification, and cluster & topic analysis.

In choosing a topic to cover, the requirements were simple; there needed to be over 5,000 articles on the website, and the topic had to be timely and relevant. In 2021, cryptocurrencies have been making big headlines. Troy, a part-time scuba diving instructor, had just gotten back from an out-of-country diving trip where a self-made crypto millionaire chartered him to instruct a private group of people.

## Methodology
### Site Scraping
With our site selected, https://news.bitcoin.com/, the first task was to scrape the site for articles. This was done with the following code:
```python
#Set url
url = "https://news.bitcoin.com/"
#Get target website
r = requests.get(url)
soup = BeautifulSoup(r.content, "html.parser")
#make file path names

#number of pages(500 x 8 articles = 4000 web pages)
numPages = 500

#page 1 of website
urls = ["https://news.bitcoin.com/"]

#make each additional website path
for i in range(2, numPages):     # The range(2, numPages) generates a list of integers from 2 to 9.
    url = "https://news.bitcoin.com/page/{}/".format(i)
    urls.append(url)

#Create directory for html files
path = "HTML_Files/Bitcoin5/"
#If path doesn't exist, create
if not os.path.isdir(path):
    os.mkdir(path)

#load web pages to directory

for url in urls:
    print(url)

    ####################################################
    # Get the content of a page
    ####################################################
    r = requests.get(url)
    soup = BeautifulSoup(r.content, "html.parser")

    ####################################################
    # Get the list of articles
    ####################################################
    h_list = soup.find_all(name="div", attrs={"class": "story story--medium"})

    for h in h_list:
        ####################################################
        # Find the anchor tag
        ####################################################
        a = h.find("a")
        b = h.find("h6")

        ####################################################
        # Extract the title & URL of an article
        ####################################################
        title = b.text
        article_url = a["href"]

        ####################################################
        # Fetch the content and save it as an HTML file
        ####################################################
        r2 = requests.get(article_url)

        file_name = article_url[len("https://news.bitcoin.com/"):-1] + ".html"
        with open(path + file_name, "w+b") as fw:
            fw.write(r2.content)

        print("- " + file_name + " saved.")

        ####################################################
        # Sleep for a second to not overload the web site
        ####################################################
        time.sleep(1)

```
After the above block of code is ran, we now have almost 5,000 html pages scraped and saved in one central location.

Now, in order to get all of the .html files into a .csv file for manual coding, the following code block was utilized:

```python
#create csv file from html files previously downloaded

with open("CSV_File/html_metadata.csv", "w", encoding="utf8") as fw:
    ####################################################
    # Column names on the first row
    ####################################################
    fw.write("file_name\tarticle_title\tarticle_date_time\tarticle_author\tarticle_text\n")   # A tab between columns and a new line between rows  

    for file_name in os.listdir(path):
        if not file_name.endswith(".html"):
            continue

        ####################################################
        # Column values starting from the second row
        ####################################################
        with open(path + file_name, "r+b") as fr:
            print(file_name)
            soup = BeautifulSoup(fr.read(), "html.parser")
            article_title = soup.find(name="article", attrs={"class": "article__body"}).find("h1").text.strip()

            ##modified by Troy -- this area contains the date of the article
            article_date_time = soup.find("main",attrs={"article full-grid"}).find("time").text.strip()

            article_text = soup.find(name="article", attrs={"class": "article__body"}).text.replace("\n", " ").replace("\t", " ").strip()

            ####################################################
            # No author exception handling
            ####################################################
            if soup.find(name="p", attrs={"class": "article__info__author"}) == None:
                article_author = ""
            else:
                article_author = soup.find(name="p", attrs={"class": "article__info__author"}).find("strong").text.strip()


            #####################################################################
            # Remove all possible tabs, as tab is being used as column delimiter
            #####################################################################
            article_title = article_title.replace("\t", "")
            article_date_time = article_date_time.replace("\t", "")

            #article_date_time = get_past_date(article_days_ago)

            article_author = article_author.replace("\t", "")
            article_text = article_text.replace("\t", "")

            fw.write("{}\t{}\t{}\t{}\t{}\n".format(file_name, article_title, article_date_time, article_author, article_text))
```

After the completion of the above code, we now have a .csv file with the name of the file, the article title, the date and time of posting, the author, and the article text. This data is ready for further processing into sentences for manual sentiment coding.

### Text Preparation and Output for Manual Coding

At this point in the process, we need to take the .csv file of article text and tokenize them into individual sentences. Made easy through the nltk package's 'sent_tokenize' function, the following line of code outputs a new panda's column in the dataframe that corresponds to a list of tokenized sentences extracted from the article_text column.
```python
df["sentences"] = df.article_text.apply(lambda x: nltk.sent_tokenize(x))
```

With a total of 161,198 sentences, a random sample needed to be selected. The following code was used to accomplish this:
```python
#Get 2000 random sentences
Random_Sentences = []
Unique_Sentences = []
pick = 2000
i = 0
while i < (pick):
    Random_Sentences.extend(random.sample(All_Sentences, 1))
    Unique_Sentences = set(Random_Sentences)
    i = len(Unique_Sentences)
Random_Sentences = list(Unique_Sentences)
```

The random sentences are then merged, duplicates removed, and output to a .csv file using Pandas.

```python
#merge list of 2000 sentences to data frame for export
dfSent = DataFrame(Random_Sentences,columns=['Sentence'])
#check for duplicates
dfSent[dfSent.Sentence.duplicated(keep="last")]
#Export to CSV to mannually classify
dfSent.to_csv("RandomSentences/Sentences2.csv",sep='\t')
```

In order to get the most accurate coding between Praveen, Troy, and I, we did some research and devised the following process for manual sentiment classification: If a sentence has more than one sentiment present, all positive, negative, and neutral adjectives and adverbs were added together and the corresponding category with the highest net count "won" the overall categorization. Sarcasm was treated as positive sentiment, as in American English, sarcasm contains positive words about a negative situation. No clear sentiment was coded as neutral.

### Sentiment Classifiers

With a .csv file of manually coded sentences ready to be used as training data, the first step is importing the file with Pandas.

```python
#import manually classified sentences
dfSent = pd.read_csv("RandomSentences/SentencesTab.csv", sep='\t', encoding = 'unicode_escape')
```

To prepare our data to be used in ML models, the first step is to utilize a sklearn vectorizer. Our X dataset will be our sentences, and our Y dataset will be our manual classifications. For this project, we will use a standard set of english stopwords as well as l2 normalization with a test size of 20%.

```python
#import vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(use_idf=True, norm="l2", stop_words="english", max_df=0.7)
X = vectorizer.fit_transform(dfSent.Sentence)
y = dfSent.Class

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
```

In order to get a clear output of the train and test scores
